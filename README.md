# Banggood Product Data Pipeline

A complete end-to-end data engineering and analysis project for scraping, processing, storing, and visualizing Banggood product data.

## ğŸ“‹ Overview

This project implements a comprehensive data pipeline that:
- Scrapes product data from Banggood across multiple categories
- Cleans and transforms the raw data
- Loads processed data into SQL Server
- Provides an interactive Streamlit dashboard for data exploration

## âœ¨ Features

- **Web Scraping**: Automated scraping of 5 categories Ã— 5 pages = 25 pages of product data using Selenium
- **Data Cleaning**: Robust data cleaning with derived feature engineering (IsHighValue, PricePerReview)
- **SQL Server Integration**: Automated data loading with validation
- **Interactive Dashboard**: Real-time data exploration with Streamlit
- **Production-Ready**: Error handling, retry logic, and progress tracking

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper   â”‚ â†’ data/raw.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cleaner   â”‚ â†’ data/cleaned_products.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Server  â”‚ â†’ Structured Database
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard  â”‚ â†’ Interactive UI
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
banggood_data_pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py          # Web scraping module
â”‚   â”œâ”€â”€ cleaner.py          # Data cleaning module
â”‚   â”œâ”€â”€ db_load.py          # SQL Server loading module
â”‚   â””â”€â”€ main_pipeline.py    # Complete ETL pipeline
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py              # Streamlit dashboard
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw.csv             # Raw scraped data
â”‚   â””â”€â”€ cleaned_products.csv # Cleaned data
â”œâ”€â”€ plots/                  # Generated visualizations (optional)
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ queries.sql         # SQL analysis queries
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- SQL Server (with database `BanggoodAnalysisDB` created)
- ODBC Driver 17 for SQL Server (or SQL Server driver)
- Chrome browser (for Selenium)

### Installation

1. **Clone or download the project**

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up SQL Server Database**
   ```sql
   CREATE DATABASE BanggoodAnalysisDB;
   
   USE BanggoodAnalysisDB;
   
   CREATE TABLE Banggood_Products (
       ProductID INT PRIMARY KEY IDENTITY(1,1),
       Category NVARCHAR(50),
       ProductName NVARCHAR(255),
       Price DECIMAL(10, 2),
       Rating DECIMAL(3, 2),
       ReviewCount INT,
       IsHighValue BIT,
       PricePerReview DECIMAL(10, 2),
       ProductURL NVARCHAR(MAX)
   );
   ```

4. **Update database connection** (if needed)
   - Edit `src/db_load.py` and `dashboard/app.py`
   - Update `SERVER` and `DATABASE` variables

## ğŸ“– How to Run

### Option 1: Run Complete Pipeline (Recommended)

Run the entire ETL pipeline in one command:

```bash
python -m src.main_pipeline
```

This will:
1. Scrape data from Banggood (5 categories Ã— 5 pages)
2. Clean and transform the data
3. Load data into SQL Server

### Option 2: Run Individual Steps

**Step 1: Scrape Data**
```bash
python -m src.scraper
```
- Scrapes 5 categories Ã— 5 pages
- Saves raw data to `data/raw.csv`
- Uses Selenium with retry logic

**Step 2: Clean Data**
```bash
python -m src.cleaner
```
- Cleans raw data
- Creates derived features (IsHighValue, PricePerReview)
- Saves cleaned data to `data/cleaned_products.csv`

**Step 3: Load to SQL Server**
```bash
python -m src.db_load
```
- Loads cleaned data into SQL Server
- Validates insertion with row counts

### Launch Dashboard

```bash
streamlit run dashboard/app.py
```
- Opens interactive dashboard in browser
- Filter by category, price range, rating
- View metrics, top products, and visualizations

## ğŸ” How Scraping Works

The scraper uses Selenium to extract product information:

1. **Categories**: RC_Toys, Smart_Home, Tools, Outdoor_Gear, Drones
2. **Pages per Category**: 5 pages (configurable)
3. **Extracted Fields**:
   - Product Name
   - Product URL
   - Price (extracted from `oriprice` attribute)
   - Rating
   - Review Count

**Features**:
- Headless Chrome browser
- Retry logic (3 attempts with exponential backoff)
- Progress bars with `tqdm`
- Respectful delays between requests

## ğŸ”„ How ETL Works

### Extract
- Selenium-based web scraping
- Data extraction using CSS selectors
- CSV export for raw data

### Transform
- **Price Cleaning**: Remove currency symbols, convert to float, fill missing with category median
- **Rating Cleaning**: Extract numeric values, default to 0
- **Review Count**: Parse text like "3 reviews" â†’ 3
- **Derived Features**:
  - `IsHighValue`: Rating â‰¥ 4.5 AND Price < category median
  - `PricePerReview`: Price / max(ReviewCount, 1)

### Load
- SQL Server insertion using `pyodbc`
- Batch processing with error handling
- Data validation and row count verification

## ğŸ“Š Dashboard Features

The Streamlit dashboard provides:

- **Category Filtering**: Select specific category or view all
- **Key Metrics**: Average price, rating, total reviews, high-value count
- **Overview Tab**: Category summary statistics
- **Top Products Tab**: Most reviewed products with configurable count
- **Visualizations Tab**: Interactive charts and saved plots
- **Value Score Tab**: Products ranked by (Rating Ã— Reviews) / Price
- **Data Explorer Tab**: Full dataset with search functionality

## âš ï¸ Important Notes

1. **Data Paths**: All CSV files are stored in the `data/` folder
2. **Database Connection**: Uses Windows Authentication by default
3. **Dashboard Fallback**: Dashboard can load from CSV if database is unavailable
4. **Scraping**: Uses headless Chrome - ensure Chrome is installed

## ğŸ”® Future Improvements

1. **Scraping**: Implement parallel scraping with `asyncio`
2. **Data Pipeline**: Add data quality checks and validation
3. **Analysis**: Add more advanced statistical analysis
4. **Dashboard**: Add more interactive filters and export functionality
5. **Infrastructure**: Containerize with Docker


**Built with**: Python, Selenium, pandas, SQL Server, Streamlit, matplotlib
